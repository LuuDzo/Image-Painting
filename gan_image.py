{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BiB72MKh554O"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils import data\n",
        "from PIL import Image\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "isHDiWa38vJi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((32,32))\n",
        "])\n",
        "dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADLM4ua3IXhn",
        "outputId": "fc7d8202-4c5e-4914-c164-7ef38ff6b1d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50000\n"
          ]
        }
      ],
      "source": [
        "  train_split, val_split, test_split = 0.6,0.2,0.2\n",
        "  train_size = int(len(dataset) * train_split)\n",
        "  val_size = int(len(dataset) * val_split)\n",
        "  test_size = int(len(dataset) * test_split)\n",
        "  print(train_size+val_size+test_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZD9TZ8EI4_w",
        "outputId": "38f28a9d-d7c8-4805-85fd-65fe7fc2978c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30000 10000 10000\n"
          ]
        }
      ],
      "source": [
        "train, val, test = data.random_split(dataset, (train_size, val_size, test_size))\n",
        "print(len(train), len(val), len(test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_LtgwEHjORu1"
      },
      "outputs": [],
      "source": [
        "def imshow(tensor):\n",
        "    if len(tensor.shape) == 4:\n",
        "        tensor = tensor[0]\n",
        "    plt.imshow(tensor[:3].permute(1, 2, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "xMiGlA6vOuT2",
        "outputId": "def760e8-e97b-44bc-f311-08c8fd94e3eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "imshow(train[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ud_Qa-NJO013"
      },
      "outputs": [],
      "source": [
        "def create_hole_mask(im_h, im_w, hole_h, hole_w):\n",
        "    i = int((im_h - hole_h + 1) * np.random.random())\n",
        "    j = int((im_w - hole_w + 1) * np.random.random())\n",
        "    mask = torch.zeros((1, im_h, im_w))\n",
        "    mask[0, i : i + hole_h, j : j + hole_w] = 1\n",
        "    return mask, (i, i + hole_h, j, j + hole_w)\n",
        "\n",
        "def create_hole_masks(N, im_h, im_w, hole_h, hole_w, same_size=True):\n",
        "    if same_size:\n",
        "        masks = [create_hole_mask(im_h, im_w, hole_h, hole_w) for _ in range(N)]\n",
        "    else:\n",
        "        h_min, h_max = hole_h\n",
        "        w_min, w_max = hole_w\n",
        "        hs, ws = np.random.randint(h_min, h_max, N), np.random.randint(w_min, w_max, N)\n",
        "        masks = [create_hole_mask(im_h, im_w, h, w) for h, w in zip(hs, ws)]\n",
        "\n",
        "    bounds = [mask[1] for mask in masks]\n",
        "    masks = [mask[0] for mask in masks]\n",
        "    masks = torch.cat(masks, dim=0)\n",
        "    return masks.unsqueeze(1), bounds\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "fZadwKwqM14b"
      },
      "outputs": [],
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.shape[0], -1)\n",
        "\n",
        "class Unflatten(nn.Module):\n",
        "\n",
        "    def __init__(self, shape):\n",
        "        super(Unflatten, self).__init__()\n",
        "        self.shape = shape\n",
        "\n",
        "    def forward(self, X):\n",
        "        return X.view(-1, *self.shape)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, im_channels):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(im_channels + 1, 64, 5, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(64),\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(128),\n",
        "\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, 3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(256),\n",
        "\n",
        "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(128),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 32, 3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, im_channels, 3, stride=1, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        out = self.net(X)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "rwalX8-6ODkY"
      },
      "outputs": [],
      "source": [
        "class LocalDiscriminator(nn.Module):\n",
        "    def __init__(self, im_channels, region_size=16):\n",
        "        super(LocalDiscriminator, self).__init__()\n",
        "\n",
        "        self.region_size = region_size\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(im_channels + 1, 64, 5, stride=2, padding=2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 128, 5, stride=2, padding=2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 256, 5, stride=2, padding=2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 512, 5, stride=2, padding=2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 5, stride=2, padding=2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            Flatten(),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm1d(1024)\n",
        "        )\n",
        "\n",
        "    def forward(self, X, mask_bounds):\n",
        "\n",
        "        local_regions = self._get_local_regions(X, mask_bounds)\n",
        "        out = self.net(local_regions)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def _get_local_regions(self, X, mask_bounds):\n",
        "\n",
        "        N, ch, im_h, im_w = X.shape\n",
        "        local_regions = torch.zeros((N, ch, self.region_size, self.region_size))\n",
        "\n",
        "        for i, bounds in enumerate(mask_bounds):\n",
        "            y1, y2, x1, x2 = bounds\n",
        "\n",
        "            ym, xm = (y1 + y2) // 2, (x1 + x2) // 2\n",
        "            y1, x1 = ym - self.region_size // 2, xm - self.region_size // 2\n",
        "            y2, x2 = y1 + self.region_size, x1 + self.region_size\n",
        "\n",
        "            if y1 < 0:\n",
        "                y1, y2 = 0, self.region_size\n",
        "            elif y2 > im_h:\n",
        "                y1, y2 = im_h - self.region_size, im_h\n",
        "\n",
        "            if x1 < 0:\n",
        "                x1, x2 = 0, self.region_size\n",
        "            elif x2 > im_w:\n",
        "                x1, x2 = im_w - self.region_size, im_w\n",
        "\n",
        "            local_regions[i, :, :, :] = X[i, :, y1 : y2, x1 : x2]\n",
        "\n",
        "        return local_regions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv_LvVCGNN8B"
      },
      "outputs": [],
      "source": [
        "# model = LocalDiscriminator(im_channels=3, region_size=14)#.cuda()\n",
        "# model.eval()\n",
        "# model.net(train[0][0][:, :14, :14].unsqueeze(0)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKBrjTHXNN1S"
      },
      "outputs": [],
      "source": [
        "# masks, bounds = create_hole_masks(1, 10, 10, 3, 2, same_size=True)\n",
        "# print(masks.shape)\n",
        "# imshow(model._get_local_regions(train[0][0].unsqueeze(0), bounds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "jW0DlObUSHMs"
      },
      "outputs": [],
      "source": [
        "class GlobalDiscriminator(nn.Module):\n",
        "    def __init__(self, im_channels):\n",
        "        super(GlobalDiscriminator, self).__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(im_channels + 1, 64, 5, stride=2, padding=2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 128, 5, stride=2, padding=2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 256, 5, stride=2, padding=2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 512, 5, stride=2, padding=2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 5, stride=2, padding=2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(512),\n",
        "#             nn.Conv2d(512, 512, 5, stride=2, padding=2),\n",
        "#             nn.LeakyReLU(0.1),\n",
        "#             nn.BatchNorm2d(512),\n",
        "            Flatten(),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm1d(1024)\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        out = self.net(X)\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jY-6SEYUSOGy"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, local_d, global_d):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.local_discriminator = local_d\n",
        "        self.global_discriminator = global_d\n",
        "\n",
        "        self.fc = nn.Linear(2048, 1)\n",
        "\n",
        "    def forward(self, X, mask_bounds):\n",
        "        local_ = self.local_discriminator(X, mask_bounds)\n",
        "        global_ = self.global_discriminator(X)\n",
        "        concated = torch.cat((local_, global_), dim=1)\n",
        "        out = self.fc(concated)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "2MW6jci-STse"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def cycle(iterable):\n",
        "    while True:\n",
        "        for x in iterable:\n",
        "            yield x\n",
        "\n",
        "def train_gan(g, d, train, val, g_o, d_o, params, masks_fn):\n",
        "\n",
        "    train_loader = data.DataLoader(train, batch_size=params['batch_size'], num_workers=0, pin_memory=True)\n",
        "    val_loader = data.DataLoader(val, params['val_batch_size'], shuffle=True, pin_memory=True)\n",
        "    val_loader = iter(cycle(val_loader))\n",
        "\n",
        "    optimizer_g = g_o\n",
        "    optimizer_d = d_o\n",
        "\n",
        "    T_c, T_d = params['T_c'], params['T_d']\n",
        "    w = params['w']\n",
        "    for epoch in range(params['epochs']):\n",
        "        ep_loss_g = 0.\n",
        "        ep_loss_d = 0.\n",
        "        fake_err = 0.\n",
        "        real_err = 0.\n",
        "\n",
        "        if epoch < T_c and epoch == 0:\n",
        "                print(f'\\n>>>> Training generator for {T_c} epochs.')\n",
        "\n",
        "        if epoch < T_c + T_d and epoch == T_c:\n",
        "                print(f'\\n>>>> Training discriminator for {T_d} epochs.')\n",
        "\n",
        "        if epoch == T_c + T_d:\n",
        "                print(f'\\n>>>> Training both generator and discriminator jointly.')\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        g.train()\n",
        "        d.train()\n",
        "        for batch, _ in train_loader:\n",
        "\n",
        "            N = batch.shape[0]\n",
        "            batch = batch\n",
        "\n",
        "            masks_g, bounds_g = masks_fn(N)\n",
        "            batch_masked = batch.clone() * (1 - masks_g)\n",
        "            batch_with_masks = torch.cat((batch_masked, masks_g[:, :1]), dim=1)\n",
        "\n",
        "            fake = g(batch_with_masks)\n",
        "\n",
        "            loss_mse = (((batch - fake) * masks_g)**2).sum() / masks_g.sum()\n",
        "\n",
        "            if epoch < T_c:\n",
        "                loss_g = loss_mse\n",
        "                loss_g.backward()\n",
        "                optimizer_g.step()\n",
        "                optimizer_g.zero_grad()\n",
        "                ep_loss_g += loss_g.detach()\n",
        "                continue\n",
        "            else:\n",
        "                inpainted = batch.clone()\n",
        "                masks_byte = masks_g.byte()\n",
        "                inpainted[masks_byte] = fake.detach()[masks_byte].view(-1)\n",
        "                inpainted = torch.cat((inpainted, masks_g[:, :1]), dim=1)\n",
        "                d_fake = d(inpainted.detach(), bounds_g)\n",
        "\n",
        "                masks_d, bounds_d = masks_fn(N)\n",
        "                real = torch.cat((batch.clone(), masks_d[:, :1]), dim=1)\n",
        "                d_real = d(real, bounds_d)\n",
        "\n",
        "                loss_d_fake = (d_fake**2).mean()\n",
        "                loss_d_real = ((d_real - 1)**2).mean()\n",
        "                loss_d = (loss_d_fake + loss_d_real) / 2\n",
        "                loss_d.backward()\n",
        "                optimizer_d.step()\n",
        "                optimizer_d.zero_grad()\n",
        "\n",
        "                if epoch >= T_c + T_d:\n",
        "                    inpainted = batch.clone()\n",
        "                    inpainted[masks_byte] = fake[masks_byte].view(-1)\n",
        "                    inpainted = torch.cat((inpainted, masks_g[:, :1]), dim=1)\n",
        "                    d_fake = d(inpainted, bounds_g)\n",
        "                    loss_g = loss_mse + w * ((d_fake - 1)**2).mean()\n",
        "\n",
        "                    loss_g.backward()\n",
        "                    optimizer_g.step()\n",
        "                    optimizer_g.zero_grad()\n",
        "\n",
        "                    ep_loss_g += loss_g.detach()\n",
        "\n",
        "                ep_loss_d += loss_d.detach()\n",
        "                fake_err += loss_d_fake.detach()\n",
        "                real_err += loss_d_real.detach()\n",
        "#             break\n",
        "\n",
        "        if not T_c <= epoch < T_c + T_d:\n",
        "            g.eval()\n",
        "            val_batch = next(val_loader)[0]\n",
        "            N, ch, _, _ = val_batch.shape\n",
        "\n",
        "            masks, _ = masks_fn(N)\n",
        "            val_batch_masked = val_batch * (1 - masks)\n",
        "            val_batch_masked = torch.cat((val_batch_masked, masks[:, :1]), dim=1)\n",
        "            val_pred = g(val_batch_masked).detach()\n",
        "            val_loss = (((val_batch - val_pred) * masks)**2).sum() / masks.sum()\n",
        "            inpainted = val_batch.clone()\n",
        "            masks_byte = masks.byte()\n",
        "            inpainted[masks_byte] = val_pred[masks_byte]\n",
        "            imgs = [\n",
        "                val_batch[0],\n",
        "                masks[0],\n",
        "                val_batch_masked[:, :-1][0],\n",
        "                val_pred[0].clamp(0, 1),\n",
        "                inpainted[0]\n",
        "            ]\n",
        "            imshow(torch.cat(imgs, dim=2))\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "        print('epoch: %d, g_loss: %0.4f, val_loss: %0.4f, d_loss: %0.4f, fake_err: %0.4f, real_err: %0.4f, time: %0.2f' %\\\n",
        "              (epoch, ep_loss_g, val_loss, ep_loss_d, fake_err, real_err, time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "1FAc8CmkSZOr"
      },
      "outputs": [],
      "source": [
        "\n",
        "global_d = GlobalDiscriminator(im_channels=3)\n",
        "local_d = LocalDiscriminator(im_channels=3, region_size=16)\n",
        "discriminator = Discriminator(local_d=local_d, global_d=global_d)\n",
        "\n",
        "generator = Generator(im_channels=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "MVD9JLpMShrU",
        "outputId": "67770598-f57c-4fc8-e5e2-cff2579be684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>>> Training both generator and discriminator jointly.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_20900\\3385780156.py:61: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at ..\\aten\\src\\ATen/native/IndexingUtils.h:28.)\n",
            "  inpainted[masks_byte] = fake.detach()[masks_byte].view(-1)\n",
            "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_20900\\3385780156.py:78: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at ..\\aten\\src\\ATen/native/IndexingUtils.h:28.)\n",
            "  inpainted[masks_byte] = fake[masks_byte].view(-1)\n",
            "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at ..\\aten\\src\\ATen/native/IndexingUtils.h:28.)\n",
            "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ASUS\\Desktop\\Image-Inpainting\\gan_image.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/Image-Inpainting/gan_image.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m optimizer_d \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(discriminator\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mtrain_params[\u001b[39m'\u001b[39m\u001b[39mlearning_rate_d\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/Image-Inpainting/gan_image.ipynb#X22sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/Image-Inpainting/gan_image.ipynb#X22sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m train_gan(generator, discriminator, train, val, optimizer_g, optimizer_d, train_params, gen_masks)\n",
            "\u001b[1;32mc:\\Users\\ASUS\\Desktop\\Image-Inpainting\\gan_image.ipynb Cell 16\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/Image-Inpainting/gan_image.ipynb#X22sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m d_fake \u001b[39m=\u001b[39m d(inpainted, bounds_g)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/Image-Inpainting/gan_image.ipynb#X22sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m loss_g \u001b[39m=\u001b[39m loss_mse \u001b[39m+\u001b[39m w \u001b[39m*\u001b[39m ((d_fake \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/Image-Inpainting/gan_image.ipynb#X22sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m loss_g\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/Image-Inpainting/gan_image.ipynb#X22sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m optimizer_g\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/Image-Inpainting/gan_image.ipynb#X22sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m optimizer_g\u001b[39m.\u001b[39mzero_grad()\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_params = {}\n",
        "train_params['w'] = 0.0005\n",
        "train_params['learning_rate_g'] = 0.0001\n",
        "train_params['learning_rate_d'] = 0.00001\n",
        "train_params['batch_size'] = 800\n",
        "train_params['val_batch_size'] = 1024\n",
        "train_params['T_c'] = 0\n",
        "train_params['T_d'] = 0\n",
        "train_params['epochs'] = 2 + train_params['T_c'] + train_params['T_d']\n",
        "\n",
        "def gen_masks(N, ch=3):\n",
        "    masks, bounds = create_hole_masks(N, 32, 32, (5, 12), (5, 12), same_size=False)\n",
        "    return masks.repeat_interleave(ch, dim=1), bounds\n",
        "\n",
        "optimizer_g = optim.Adam(generator.parameters(), lr=train_params['learning_rate_g'])\n",
        "optimizer_d = optim.Adam(discriminator.parameters(), lr=train_params['learning_rate_d'])\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "train_gan(generator, discriminator, train, val, optimizer_g, optimizer_d, train_params, gen_masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "DdRF82a0Sm_-"
      },
      "outputs": [],
      "source": [
        "\n",
        "loss = 0\n",
        "pixels = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "id": "B6PPRDL7SrxX",
        "outputId": "70f1872f-0fd0-44cb-9664-68fe443759d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x00000271293F3BD0>\n",
            "512\n",
            "tensor([7, 0, 1, 3, 0, 7, 6, 7, 7, 3, 0, 7, 5, 8, 9, 0, 8, 7, 3, 3, 3, 3, 0, 4,\n",
            "        5, 3, 5, 8, 6, 6, 6, 9, 0, 1, 7, 0, 3, 4, 3, 8, 5, 2, 1, 9, 4, 1, 9, 3,\n",
            "        6, 6, 2, 7, 4, 3, 6, 9, 1, 3, 0, 8, 8, 7, 1, 9, 4, 0, 1, 4, 7, 0, 2, 0,\n",
            "        9, 6, 2, 5, 3, 8, 0, 3, 4, 9, 5, 7, 4, 2, 1, 0, 3, 9, 4, 0, 5, 3, 6, 7,\n",
            "        0, 4, 7, 8, 2, 5, 7, 0, 0, 2, 2, 9, 2, 0, 1, 8, 5, 4, 0, 0, 0, 5, 4, 6,\n",
            "        5, 5, 1, 9, 0, 9, 4, 8, 8, 7, 7, 3, 7, 0, 1, 6, 7, 2, 1, 1, 2, 5, 5, 0,\n",
            "        9, 5, 8, 1, 2, 2, 4, 7, 3, 2, 5, 0, 3, 9, 8, 6, 5, 0, 7, 3, 0, 9, 4, 2,\n",
            "        4, 1, 7, 8, 6, 6, 5, 5, 5, 0, 0, 8, 1, 0, 0, 1, 9, 4, 1, 4, 3, 6, 2, 5,\n",
            "        3, 3, 4, 2, 0, 1, 7, 5, 9, 3, 3, 9, 0, 6, 6, 9, 9, 7, 3, 0, 0, 7, 0, 4,\n",
            "        8, 8, 9, 8, 9, 6, 7, 6, 2, 6, 3, 5, 3, 9, 4, 7, 4, 7, 0, 3, 0, 4, 4, 6,\n",
            "        8, 5, 4, 6, 6, 6, 6, 5, 8, 6, 3, 8, 4, 3, 5, 5, 3, 0, 3, 5, 6, 5, 5, 8,\n",
            "        5, 4, 0, 4, 9, 1, 8, 7, 3, 1, 6, 1, 3, 9, 5, 4, 0, 3, 3, 1, 6, 9, 7, 9,\n",
            "        1, 0, 8, 3, 0, 9, 6, 8, 9, 0, 7, 2, 6, 7, 9, 0, 2, 2, 8, 4, 9, 4, 0, 5,\n",
            "        4, 9, 8, 3, 6, 3, 4, 9, 7, 1, 2, 7, 1, 7, 8, 2, 2, 9, 5, 6, 8, 2, 5, 6,\n",
            "        1, 7, 4, 2, 4, 8, 9, 9, 3, 5, 9, 4, 9, 2, 7, 2, 3, 9, 0, 5, 5, 3, 7, 5,\n",
            "        5, 0, 7, 5, 0, 6, 8, 4, 5, 0, 7, 0, 0, 9, 1, 8, 3, 4, 8, 3, 8, 8, 1, 4,\n",
            "        0, 0, 7, 9, 8, 5, 0, 1, 8, 8, 1, 4, 6, 7, 7, 9, 8, 5, 3, 4, 7, 8, 5, 7,\n",
            "        3, 5, 2, 5, 8, 2, 2, 5, 8, 4, 4, 6, 3, 8, 8, 7, 7, 4, 3, 2, 5, 3, 0, 5,\n",
            "        2, 2, 0, 1, 5, 6, 3, 1, 0, 3, 7, 7, 1, 9, 7, 9, 1, 0, 7, 9, 2, 1, 0, 1,\n",
            "        0, 0, 1, 0, 2, 9, 6, 3, 8, 1, 0, 6, 7, 4, 1, 6, 9, 0, 7, 5, 6, 6, 9, 9,\n",
            "        0, 8, 3, 9, 0, 2, 0, 0, 8, 3, 4, 6, 1, 1, 0, 7, 7, 8, 3, 3, 2, 3, 0, 7,\n",
            "        9, 8, 4, 6, 5, 5, 0, 3])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_20900\\1526891457.py:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at ..\\aten\\src\\ATen/native/IndexingUtils.h:28.)\n",
            "  inpainted[masks_byte] = pred[masks_byte].view(-1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "512\n",
            "tensor([3, 3, 2, 3, 1, 8, 5, 5, 7, 8, 8, 3, 2, 2, 8, 0, 0, 5, 9, 1, 5, 0, 5, 0,\n",
            "        5, 2, 5, 9, 5, 5, 8, 3, 6, 6, 5, 6, 7, 9, 7, 6, 5, 9, 3, 8, 0, 1, 6, 1,\n",
            "        4, 5, 3, 9, 2, 7, 3, 8, 5, 0, 1, 1, 3, 5, 2, 0, 0, 0, 0, 7, 6, 1, 5, 3,\n",
            "        5, 4, 0, 7, 4, 6, 8, 3, 0, 9, 7, 9, 4, 3, 7, 9, 1, 6, 0, 1, 9, 1, 2, 9,\n",
            "        8, 5, 8, 9, 8, 0, 0, 1, 7, 3, 8, 0, 7, 2, 9, 6, 3, 2, 2, 9, 4, 7, 9, 3,\n",
            "        8, 5, 8, 0, 2, 0, 9, 8, 4, 4, 6, 4, 5, 5, 0, 4, 0, 8, 0, 6, 7, 8, 8, 6,\n",
            "        9, 4, 7, 6, 6, 4, 2, 4, 2, 8, 6, 1, 0, 3, 8, 9, 2, 1, 1, 1, 3, 1, 2, 8,\n",
            "        2, 6, 3, 7, 6, 3, 7, 0, 7, 1, 9, 0, 0, 9, 2, 9, 0, 7, 3, 4, 8, 7, 7, 7,\n",
            "        2, 5, 3, 9, 1, 1, 7, 7, 1, 2, 2, 2, 1, 9, 7, 2, 0, 6, 8, 5, 0, 7, 8, 0,\n",
            "        0, 3, 4, 3, 5, 1, 8, 3, 6, 0, 6, 8, 8, 6, 8, 6, 3, 4, 5, 3, 3, 9, 9, 8,\n",
            "        2, 1, 2, 5, 4, 3, 7, 4, 6, 1, 6, 4, 9, 8, 8, 4, 7, 4, 5, 9, 6, 1, 4, 9,\n",
            "        0, 6, 1, 5, 7, 9, 3, 1, 4, 5, 2, 8, 7, 0, 3, 5, 9, 9, 4, 1, 9, 3, 1, 8,\n",
            "        1, 1, 0, 7, 0, 7, 9, 9, 4, 0, 9, 5, 5, 5, 5, 7, 3, 1, 0, 8, 6, 5, 4, 0,\n",
            "        4, 9, 5, 9, 0, 8, 4, 5, 7, 0, 6, 9, 9, 4, 1, 3, 2, 7, 2, 0, 3, 5, 2, 6,\n",
            "        8, 4, 8, 7, 5, 0, 9, 9, 5, 2, 7, 4, 4, 9, 1, 3, 6, 5, 3, 8, 2, 8, 8, 7,\n",
            "        5, 9, 6, 8, 2, 7, 8, 3, 9, 6, 4, 1, 8, 0, 7, 0, 9, 8, 2, 0, 5, 0, 1, 6,\n",
            "        9, 9, 1, 0, 3, 8, 8, 1, 8, 4, 9, 5, 7, 5, 7, 7, 5, 5, 6, 4, 2, 9, 3, 0,\n",
            "        6, 0, 2, 4, 6, 9, 5, 6, 0, 7, 9, 6, 2, 2, 2, 5, 2, 7, 3, 3, 5, 5, 8, 8,\n",
            "        0, 1, 2, 5, 6, 6, 0, 1, 5, 2, 9, 5, 2, 8, 7, 1, 9, 8, 5, 0, 1, 5, 8, 8,\n",
            "        4, 6, 5, 2, 5, 5, 8, 5, 2, 0, 7, 2, 1, 1, 4, 0, 3, 4, 7, 9, 7, 0, 8, 0,\n",
            "        0, 2, 4, 3, 5, 6, 4, 9, 4, 1, 4, 1, 2, 6, 2, 1, 6, 3, 8, 9, 8, 2, 8, 5,\n",
            "        8, 5, 8, 3, 0, 1, 6, 5])\n",
            "512\n",
            "tensor([1, 5, 7, 8, 4, 5, 4, 1, 9, 7, 1, 7, 6, 5, 4, 3, 8, 4, 3, 2, 4, 7, 6, 8,\n",
            "        3, 3, 3, 1, 4, 5, 7, 3, 8, 8, 9, 8, 3, 1, 3, 9, 5, 8, 8, 4, 5, 3, 2, 5,\n",
            "        6, 3, 7, 3, 3, 4, 5, 1, 9, 0, 8, 3, 1, 9, 8, 0, 3, 3, 6, 8, 7, 0, 9, 0,\n",
            "        3, 7, 4, 1, 0, 8, 2, 1, 6, 3, 4, 1, 5, 4, 8, 3, 0, 7, 1, 2, 9, 0, 7, 9,\n",
            "        3, 7, 3, 9, 1, 8, 8, 8, 3, 4, 8, 5, 0, 1, 9, 4, 8, 3, 2, 8, 3, 2, 5, 3,\n",
            "        6, 8, 4, 7, 3, 6, 9, 7, 5, 7, 8, 7, 4, 9, 3, 3, 9, 9, 5, 3, 4, 8, 2, 4,\n",
            "        6, 4, 2, 5, 8, 2, 9, 5, 3, 4, 9, 8, 5, 5, 1, 6, 7, 6, 4, 2, 2, 8, 8, 9,\n",
            "        1, 1, 0, 0, 1, 6, 1, 6, 2, 0, 4, 0, 3, 0, 9, 7, 3, 1, 9, 9, 2, 8, 9, 0,\n",
            "        9, 3, 4, 2, 9, 0, 3, 5, 0, 2, 3, 8, 3, 7, 6, 0, 6, 1, 4, 8, 9, 4, 1, 7,\n",
            "        9, 2, 1, 7, 4, 8, 3, 5, 7, 6, 2, 7, 0, 2, 5, 2, 9, 8, 1, 9, 0, 5, 0, 7,\n",
            "        9, 6, 6, 4, 2, 2, 2, 5, 7, 5, 5, 7, 3, 2, 7, 4, 8, 7, 5, 5, 2, 7, 1, 0,\n",
            "        3, 1, 2, 4, 2, 5, 1, 7, 7, 7, 2, 8, 8, 5, 0, 1, 1, 0, 1, 9, 4, 7, 8, 9,\n",
            "        2, 8, 9, 9, 3, 1, 1, 3, 1, 2, 6, 5, 8, 6, 4, 2, 2, 6, 8, 1, 2, 5, 0, 4,\n",
            "        7, 8, 4, 3, 5, 6, 6, 7, 1, 2, 9, 7, 4, 7, 8, 6, 8, 2, 5, 8, 8, 1, 4, 6,\n",
            "        2, 5, 7, 5, 7, 5, 3, 3, 3, 1, 0, 5, 8, 9, 0, 3, 6, 9, 4, 2, 6, 1, 3, 3,\n",
            "        8, 6, 2, 7, 7, 4, 4, 9, 8, 6, 1, 8, 5, 5, 6, 2, 8, 5, 1, 6, 4, 9, 8, 4,\n",
            "        0, 6, 9, 3, 5, 0, 7, 5, 5, 0, 2, 9, 8, 6, 5, 5, 3, 3, 4, 7, 5, 8, 5, 4,\n",
            "        7, 1, 6, 6, 6, 5, 9, 7, 5, 8, 2, 2, 5, 8, 1, 8, 1, 8, 0, 9, 2, 5, 2, 1,\n",
            "        2, 1, 4, 1, 7, 4, 9, 0, 7, 8, 2, 1, 2, 7, 6, 2, 0, 0, 7, 3, 4, 5, 8, 1,\n",
            "        6, 6, 5, 8, 9, 3, 7, 2, 7, 9, 4, 2, 0, 5, 7, 8, 4, 3, 8, 8, 9, 4, 0, 8,\n",
            "        8, 7, 2, 7, 9, 2, 9, 7, 4, 7, 6, 5, 4, 5, 3, 2, 8, 6, 6, 3, 1, 5, 4, 3,\n",
            "        9, 9, 0, 2, 7, 0, 8, 5])\n",
            "512\n",
            "tensor([1, 3, 5, 6, 0, 6, 5, 7, 7, 1, 5, 4, 8, 0, 0, 6, 1, 6, 5, 9, 8, 0, 4, 4,\n",
            "        9, 0, 6, 5, 0, 9, 4, 6, 2, 5, 6, 1, 4, 9, 7, 4, 5, 4, 2, 3, 5, 7, 6, 2,\n",
            "        7, 8, 2, 2, 2, 0, 1, 0, 5, 2, 6, 4, 6, 3, 2, 9, 4, 3, 5, 9, 4, 3, 0, 2,\n",
            "        3, 6, 4, 3, 0, 4, 7, 1, 4, 2, 2, 6, 6, 0, 9, 4, 0, 2, 5, 3, 8, 5, 8, 7,\n",
            "        9, 6, 3, 7, 8, 5, 6, 8, 9, 5, 1, 5, 3, 0, 2, 8, 4, 5, 5, 7, 6, 1, 1, 2,\n",
            "        8, 4, 1, 6, 8, 5, 2, 6, 4, 0, 0, 3, 4, 7, 1, 4, 2, 5, 0, 1, 4, 8, 5, 1,\n",
            "        7, 3, 3, 2, 1, 5, 3, 1, 9, 3, 3, 9, 9, 5, 0, 8, 6, 3, 3, 8, 6, 7, 6, 1,\n",
            "        2, 5, 0, 1, 0, 5, 2, 3, 0, 3, 6, 6, 6, 1, 8, 4, 5, 1, 6, 2, 9, 3, 3, 9,\n",
            "        8, 1, 1, 2, 6, 6, 6, 4, 3, 5, 8, 7, 9, 9, 6, 5, 8, 0, 0, 2, 3, 9, 2, 5,\n",
            "        2, 4, 5, 1, 4, 0, 1, 6, 9, 7, 0, 9, 4, 1, 4, 2, 3, 4, 7, 5, 1, 8, 5, 5,\n",
            "        0, 8, 6, 2, 1, 0, 0, 9, 6, 2, 6, 8, 3, 2, 8, 0, 7, 1, 3, 6, 7, 7, 8, 6,\n",
            "        4, 4, 8, 3, 5, 4, 5, 2, 0, 5, 2, 5, 8, 7, 2, 6, 4, 3, 8, 4, 1, 2, 5, 3,\n",
            "        4, 6, 7, 8, 1, 5, 8, 6, 5, 0, 0, 3, 6, 5, 3, 4, 6, 6, 1, 8, 0, 3, 0, 6,\n",
            "        3, 5, 0, 5, 3, 3, 5, 3, 7, 7, 6, 3, 2, 1, 5, 4, 3, 0, 5, 6, 4, 6, 3, 0,\n",
            "        9, 8, 5, 4, 4, 8, 6, 4, 9, 6, 6, 1, 3, 0, 4, 9, 5, 3, 0, 1, 9, 4, 6, 2,\n",
            "        7, 1, 0, 4, 5, 9, 9, 0, 9, 8, 4, 9, 5, 5, 4, 5, 5, 9, 7, 7, 8, 1, 6, 3,\n",
            "        6, 2, 5, 2, 3, 8, 6, 9, 6, 4, 1, 5, 9, 3, 1, 8, 9, 8, 7, 2, 2, 1, 8, 0,\n",
            "        7, 3, 8, 9, 5, 7, 0, 9, 1, 7, 9, 3, 2, 5, 0, 1, 3, 4, 9, 1, 1, 1, 3, 2,\n",
            "        6, 8, 2, 4, 4, 6, 7, 8, 3, 1, 2, 1, 2, 8, 6, 0, 4, 0, 7, 8, 5, 3, 7, 9,\n",
            "        3, 5, 2, 5, 9, 5, 2, 7, 4, 0, 1, 1, 5, 7, 7, 5, 4, 3, 8, 2, 7, 2, 6, 0,\n",
            "        3, 2, 9, 3, 7, 1, 0, 7, 9, 3, 2, 3, 6, 5, 1, 3, 3, 4, 7, 3, 3, 4, 2, 8,\n",
            "        6, 1, 9, 5, 2, 1, 3, 8])\n",
            "512\n",
            "tensor([6, 7, 0, 2, 1, 8, 1, 3, 3, 4, 6, 5, 4, 3, 9, 5, 3, 3, 0, 6, 0, 9, 4, 5,\n",
            "        5, 0, 8, 9, 5, 1, 6, 3, 3, 1, 5, 2, 9, 0, 3, 6, 5, 2, 6, 0, 6, 8, 6, 6,\n",
            "        4, 2, 4, 1, 0, 4, 8, 3, 9, 6, 6, 4, 2, 1, 0, 6, 1, 7, 9, 9, 6, 5, 7, 9,\n",
            "        2, 4, 5, 8, 1, 4, 5, 6, 2, 2, 6, 8, 6, 1, 4, 5, 0, 0, 0, 8, 6, 0, 5, 2,\n",
            "        4, 9, 6, 5, 5, 0, 5, 3, 0, 6, 7, 8, 2, 4, 2, 3, 4, 1, 1, 3, 1, 3, 4, 8,\n",
            "        2, 8, 0, 3, 5, 5, 0, 2, 4, 1, 3, 7, 8, 3, 5, 7, 6, 2, 4, 3, 0, 5, 8, 9,\n",
            "        3, 9, 4, 6, 8, 2, 1, 4, 4, 6, 7, 5, 9, 8, 1, 9, 0, 3, 2, 7, 9, 7, 9, 6,\n",
            "        7, 3, 2, 6, 1, 3, 9, 2, 9, 6, 6, 4, 9, 3, 0, 0, 0, 1, 6, 3, 0, 0, 2, 4,\n",
            "        1, 9, 4, 8, 6, 5, 8, 0, 4, 0, 4, 8, 5, 3, 4, 4, 9, 6, 9, 6, 8, 3, 5, 7,\n",
            "        3, 5, 3, 3, 8, 8, 2, 6, 8, 5, 9, 1, 5, 7, 6, 1, 9, 5, 8, 3, 3, 3, 2, 5,\n",
            "        3, 3, 1, 0, 0, 2, 1, 3, 6, 2, 5, 3, 5, 5, 2, 3, 0, 6, 6, 9, 2, 9, 1, 1,\n",
            "        5, 6, 0, 6, 9, 7, 1, 7, 5, 1, 5, 2, 0, 3, 5, 8, 5, 4, 5, 3, 4, 2, 8, 9,\n",
            "        2, 9, 1, 6, 0, 3, 3, 7, 9, 8, 6, 1, 2, 9, 6, 9, 9, 1, 9, 3, 9, 8, 4, 5,\n",
            "        7, 9, 8, 5, 8, 6, 4, 4, 7, 8, 5, 9, 1, 3, 7, 2, 3, 0, 2, 1, 0, 0, 7, 4,\n",
            "        5, 8, 2, 3, 0, 7, 2, 5, 1, 5, 5, 7, 5, 9, 9, 5, 3, 4, 6, 5, 7, 7, 4, 4,\n",
            "        2, 3, 1, 9, 5, 0, 3, 2, 0, 8, 9, 4, 9, 8, 6, 6, 7, 8, 3, 2, 3, 1, 3, 9,\n",
            "        1, 5, 9, 7, 3, 7, 7, 5, 5, 4, 7, 7, 6, 7, 8, 1, 7, 4, 9, 0, 8, 5, 4, 8,\n",
            "        7, 4, 1, 0, 0, 8, 8, 1, 5, 3, 4, 6, 1, 3, 3, 9, 1, 0, 6, 5, 6, 4, 8, 7,\n",
            "        1, 1, 5, 1, 6, 0, 0, 5, 5, 7, 5, 9, 2, 1, 2, 1, 6, 0, 0, 1, 4, 3, 3, 1,\n",
            "        8, 5, 9, 3, 9, 2, 5, 3, 8, 3, 0, 7, 9, 6, 7, 6, 2, 4, 1, 4, 1, 3, 1, 1,\n",
            "        1, 2, 6, 9, 6, 8, 1, 3, 3, 7, 9, 2, 4, 9, 5, 5, 5, 5, 8, 2, 7, 0, 1, 1,\n",
            "        4, 0, 7, 0, 6, 6, 7, 1])\n",
            "512\n",
            "tensor([9, 0, 7, 1, 1, 0, 7, 6, 7, 6, 5, 0, 5, 8, 5, 5, 6, 6, 3, 8, 6, 9, 1, 5,\n",
            "        7, 2, 2, 3, 8, 5, 5, 5, 2, 0, 8, 5, 6, 1, 3, 9, 6, 5, 3, 8, 4, 5, 4, 3,\n",
            "        3, 0, 0, 8, 5, 2, 4, 3, 4, 3, 1, 3, 8, 4, 7, 8, 8, 8, 3, 9, 9, 9, 9, 7,\n",
            "        1, 5, 6, 1, 3, 0, 5, 3, 9, 6, 7, 0, 4, 4, 0, 4, 9, 9, 4, 7, 7, 0, 9, 5,\n",
            "        3, 5, 2, 1, 5, 3, 8, 3, 4, 1, 2, 5, 3, 4, 5, 3, 3, 2, 4, 5, 9, 8, 7, 4,\n",
            "        4, 5, 4, 5, 9, 6, 0, 4, 5, 5, 6, 8, 2, 2, 1, 7, 0, 2, 2, 1, 0, 0, 9, 3,\n",
            "        9, 2, 4, 0, 8, 8, 3, 7, 3, 1, 0, 8, 2, 4, 4, 1, 8, 0, 1, 2, 9, 7, 6, 3,\n",
            "        8, 4, 9, 9, 7, 2, 0, 0, 3, 6, 3, 4, 3, 6, 9, 9, 3, 3, 0, 9, 5, 3, 7, 6,\n",
            "        2, 5, 2, 3, 8, 8, 6, 0, 8, 1, 3, 2, 3, 6, 9, 6, 8, 9, 8, 9, 2, 0, 6, 2,\n",
            "        6, 6, 4, 7, 5, 1, 1, 0, 5, 1, 6, 3, 0, 4, 3, 6, 1, 6, 2, 0, 9, 3, 7, 7,\n",
            "        3, 7, 4, 7, 2, 9, 0, 6, 1, 1, 5, 9, 5, 9, 4, 8, 7, 1, 6, 8, 3, 9, 3, 1,\n",
            "        4, 6, 0, 5, 9, 9, 7, 8, 5, 1, 8, 6, 2, 3, 8, 8, 9, 4, 5, 2, 8, 7, 0, 7,\n",
            "        2, 0, 1, 0, 2, 6, 7, 4, 3, 0, 5, 2, 6, 9, 6, 7, 4, 8, 1, 8, 0, 3, 3, 0,\n",
            "        5, 9, 8, 8, 8, 8, 7, 0, 4, 2, 4, 7, 5, 5, 5, 1, 4, 8, 8, 5, 8, 5, 1, 6,\n",
            "        6, 3, 5, 2, 6, 8, 4, 2, 7, 6, 6, 3, 7, 0, 2, 8, 5, 1, 5, 6, 4, 8, 8, 6,\n",
            "        2, 2, 0, 8, 9, 1, 0, 6, 6, 6, 2, 6, 7, 0, 0, 7, 7, 8, 4, 2, 1, 6, 0, 3,\n",
            "        3, 8, 0, 6, 7, 7, 3, 0, 7, 8, 9, 5, 9, 2, 8, 4, 5, 2, 7, 1, 7, 6, 5, 7,\n",
            "        0, 6, 8, 1, 2, 1, 5, 0, 1, 8, 9, 2, 4, 6, 8, 6, 5, 9, 3, 8, 2, 9, 3, 5,\n",
            "        7, 3, 2, 8, 0, 8, 7, 9, 3, 3, 3, 1, 0, 4, 8, 8, 9, 9, 1, 0, 5, 9, 3, 4,\n",
            "        2, 8, 2, 7, 9, 2, 0, 8, 7, 5, 6, 7, 4, 4, 0, 0, 3, 1, 0, 4, 2, 1, 2, 0,\n",
            "        8, 0, 4, 4, 4, 7, 9, 7, 2, 8, 4, 2, 0, 0, 1, 0, 7, 2, 7, 9, 3, 6, 0, 6,\n",
            "        5, 8, 6, 3, 2, 4, 0, 9])\n",
            "512\n",
            "tensor([7, 4, 5, 3, 3, 1, 2, 9, 2, 1, 3, 7, 0, 5, 5, 9, 6, 5, 4, 9, 5, 1, 6, 4,\n",
            "        6, 9, 1, 7, 8, 4, 7, 4, 6, 3, 0, 2, 5, 7, 8, 5, 6, 1, 9, 4, 7, 5, 1, 5,\n",
            "        5, 1, 8, 8, 5, 3, 9, 7, 5, 7, 5, 9, 0, 0, 8, 0, 3, 9, 6, 3, 8, 1, 7, 6,\n",
            "        8, 4, 3, 1, 5, 5, 5, 6, 4, 3, 3, 4, 3, 7, 8, 4, 0, 4, 5, 7, 0, 2, 0, 0,\n",
            "        4, 4, 7, 8, 8, 0, 3, 5, 7, 0, 9, 3, 1, 6, 2, 0, 8, 9, 2, 0, 8, 8, 1, 6,\n",
            "        4, 4, 4, 9, 8, 2, 8, 7, 7, 6, 8, 3, 1, 3, 1, 0, 8, 7, 6, 8, 7, 3, 4, 8,\n",
            "        2, 0, 7, 8, 1, 8, 8, 3, 3, 8, 5, 9, 6, 7, 7, 5, 0, 6, 0, 8, 1, 3, 6, 9,\n",
            "        6, 6, 8, 0, 5, 2, 9, 8, 4, 8, 5, 6, 9, 1, 2, 0, 0, 6, 9, 6, 7, 6, 4, 4,\n",
            "        9, 9, 7, 3, 0, 4, 2, 4, 7, 9, 5, 7, 6, 8, 7, 3, 1, 9, 0, 8, 5, 0, 5, 4,\n",
            "        5, 6, 4, 6, 8, 3, 4, 3, 4, 8, 7, 7, 0, 4, 2, 1, 4, 0, 6, 9, 8, 9, 4, 3,\n",
            "        1, 0, 4, 7, 6, 4, 7, 1, 2, 0, 1, 5, 7, 6, 5, 0, 7, 0, 1, 1, 2, 7, 6, 9,\n",
            "        1, 8, 3, 0, 7, 0, 2, 0, 8, 9, 9, 4, 8, 2, 3, 1, 1, 3, 4, 4, 1, 0, 9, 0,\n",
            "        1, 7, 9, 4, 4, 6, 8, 8, 2, 6, 1, 1, 6, 9, 6, 9, 7, 5, 6, 5, 7, 0, 8, 1,\n",
            "        0, 5, 8, 0, 9, 4, 8, 1, 8, 7, 5, 2, 3, 3, 9, 6, 3, 5, 3, 2, 5, 7, 2, 3,\n",
            "        9, 2, 1, 2, 9, 5, 5, 4, 4, 3, 4, 0, 1, 6, 1, 7, 5, 3, 7, 0, 1, 8, 7, 1,\n",
            "        2, 6, 4, 2, 5, 4, 2, 9, 2, 5, 8, 3, 2, 2, 9, 2, 7, 4, 1, 4, 2, 4, 2, 9,\n",
            "        5, 6, 4, 2, 7, 0, 9, 8, 4, 4, 1, 1, 4, 3, 7, 7, 8, 1, 5, 7, 7, 9, 8, 3,\n",
            "        6, 8, 8, 1, 5, 5, 1, 3, 5, 2, 2, 5, 5, 4, 9, 1, 3, 8, 4, 2, 3, 7, 5, 2,\n",
            "        3, 7, 7, 8, 7, 5, 0, 1, 2, 0, 0, 6, 5, 8, 2, 6, 6, 3, 1, 8, 3, 7, 4, 2,\n",
            "        1, 8, 8, 5, 1, 4, 8, 2, 3, 4, 4, 7, 6, 1, 3, 2, 1, 2, 5, 1, 3, 3, 5, 3,\n",
            "        5, 4, 1, 0, 1, 1, 3, 9, 8, 4, 8, 8, 5, 3, 9, 2, 8, 4, 9, 1, 9, 9, 3, 5,\n",
            "        9, 3, 9, 8, 2, 0, 4, 6])\n",
            "512\n",
            "tensor([4, 7, 2, 1, 8, 0, 2, 5, 7, 4, 4, 7, 5, 1, 1, 3, 0, 9, 0, 1, 7, 0, 3, 5,\n",
            "        4, 0, 7, 0, 0, 0, 5, 2, 0, 5, 3, 5, 2, 0, 1, 7, 4, 4, 1, 8, 5, 8, 4, 5,\n",
            "        4, 6, 3, 3, 1, 1, 2, 3, 0, 4, 0, 0, 9, 6, 7, 9, 3, 3, 5, 0, 7, 3, 4, 6,\n",
            "        7, 9, 9, 9, 3, 0, 1, 1, 4, 1, 9, 9, 3, 2, 6, 7, 1, 0, 4, 4, 0, 1, 7, 0,\n",
            "        6, 4, 0, 8, 1, 7, 3, 2, 8, 0, 4, 0, 8, 9, 3, 7, 5, 9, 4, 4, 3, 8, 4, 9,\n",
            "        0, 5, 2, 7, 2, 9, 4, 0, 8, 3, 9, 7, 3, 5, 5, 0, 4, 4, 7, 5, 8, 1, 0, 5,\n",
            "        8, 5, 5, 1, 9, 8, 2, 3, 1, 4, 5, 2, 7, 0, 6, 4, 0, 5, 0, 6, 8, 5, 0, 2,\n",
            "        8, 1, 8, 2, 9, 5, 7, 5, 4, 1, 7, 8, 5, 8, 5, 9, 4, 2, 6, 8, 4, 5, 4, 3,\n",
            "        0, 0, 7, 7, 7, 6, 8, 2, 4, 2, 0, 0, 8, 5, 7, 8, 0, 2, 6, 6, 4, 7, 5, 2,\n",
            "        4, 3, 4, 3, 6, 9, 7, 4, 0, 3, 5, 4, 3, 8, 0, 9, 6, 7, 6, 3, 4, 0, 7, 2,\n",
            "        7, 2, 9, 8, 1, 0, 0, 6, 1, 6, 4, 5, 7, 3, 5, 7, 5, 5, 1, 5, 2, 2, 2, 2,\n",
            "        0, 5, 7, 7, 1, 6, 4, 4, 5, 1, 5, 5, 9, 6, 6, 9, 6, 4, 3, 2, 5, 1, 8, 6,\n",
            "        3, 3, 6, 3, 1, 8, 5, 0, 6, 2, 0, 9, 3, 3, 2, 1, 2, 8, 8, 0, 2, 6, 4, 7,\n",
            "        8, 0, 6, 9, 2, 9, 1, 1, 5, 7, 3, 5, 4, 4, 0, 3, 3, 7, 1, 2, 1, 3, 2, 4,\n",
            "        1, 6, 1, 1, 5, 2, 4, 5, 7, 2, 7, 1, 8, 4, 6, 9, 8, 6, 3, 3, 6, 9, 6, 3,\n",
            "        0, 2, 3, 3, 0, 2, 9, 3, 9, 0, 3, 9, 0, 6, 8, 1, 4, 2, 4, 0, 2, 5, 8, 8,\n",
            "        9, 6, 0, 1, 5, 4, 7, 4, 1, 4, 2, 0, 3, 0, 6, 3, 7, 0, 8, 9, 8, 4, 3, 3,\n",
            "        4, 7, 2, 9, 7, 5, 9, 7, 4, 3, 7, 8, 1, 7, 3, 9, 9, 8, 2, 5, 2, 1, 6, 4,\n",
            "        9, 6, 8, 5, 4, 8, 5, 3, 6, 0, 3, 9, 9, 4, 6, 6, 0, 1, 4, 5, 8, 5, 1, 5,\n",
            "        3, 4, 1, 0, 7, 9, 5, 9, 2, 6, 6, 7, 5, 8, 1, 7, 3, 8, 0, 9, 4, 4, 9, 6,\n",
            "        2, 7, 2, 9, 8, 4, 9, 8, 8, 6, 6, 4, 5, 2, 9, 9, 4, 9, 4, 2, 9, 7, 0, 6,\n",
            "        6, 0, 3, 7, 9, 3, 4, 9])\n",
            "512\n",
            "tensor([7, 4, 0, 2, 2, 5, 4, 1, 4, 2, 5, 9, 8, 0, 3, 7, 5, 8, 5, 3, 3, 8, 1, 3,\n",
            "        3, 2, 7, 8, 1, 4, 1, 9, 1, 1, 6, 1, 2, 3, 9, 7, 9, 1, 3, 9, 4, 2, 6, 9,\n",
            "        2, 0, 8, 1, 2, 8, 5, 3, 8, 6, 0, 1, 3, 5, 9, 3, 0, 0, 1, 7, 5, 3, 8, 7,\n",
            "        4, 4, 8, 7, 3, 7, 3, 6, 6, 8, 2, 6, 8, 6, 7, 2, 7, 7, 6, 6, 5, 7, 5, 2,\n",
            "        4, 0, 3, 8, 4, 7, 4, 2, 3, 2, 5, 1, 0, 7, 4, 3, 1, 9, 9, 6, 4, 5, 2, 3,\n",
            "        1, 1, 3, 6, 3, 7, 8, 3, 4, 2, 1, 7, 5, 5, 9, 5, 6, 9, 7, 5, 5, 0, 5, 0,\n",
            "        9, 9, 1, 9, 1, 7, 6, 8, 5, 8, 6, 3, 3, 6, 1, 8, 9, 2, 3, 9, 4, 4, 8, 4,\n",
            "        7, 9, 8, 3, 6, 0, 3, 5, 2, 3, 3, 9, 5, 5, 2, 5, 5, 1, 1, 6, 8, 7, 6, 6,\n",
            "        6, 0, 6, 8, 3, 5, 4, 5, 3, 5, 7, 1, 1, 6, 1, 2, 9, 3, 1, 8, 0, 3, 5, 3,\n",
            "        5, 3, 3, 4, 9, 4, 4, 9, 1, 9, 7, 7, 1, 8, 9, 7, 8, 4, 6, 0, 6, 6, 4, 8,\n",
            "        3, 7, 5, 9, 7, 2, 3, 5, 5, 9, 3, 6, 0, 9, 5, 9, 3, 4, 0, 9, 3, 7, 2, 6,\n",
            "        4, 9, 6, 6, 0, 4, 2, 4, 4, 2, 1, 6, 1, 5, 9, 6, 1, 5, 0, 9, 1, 6, 7, 9,\n",
            "        2, 6, 9, 0, 6, 2, 7, 9, 3, 2, 9, 5, 2, 6, 6, 5, 3, 3, 9, 2, 0, 7, 0, 8,\n",
            "        1, 1, 2, 0, 8, 3, 9, 0, 1, 4, 1, 1, 9, 1, 9, 1, 1, 7, 3, 2, 6, 0, 8, 6,\n",
            "        8, 4, 2, 5, 4, 9, 8, 5, 3, 4, 4, 7, 3, 7, 5, 1, 3, 7, 0, 6, 7, 9, 9, 5,\n",
            "        9, 9, 9, 3, 7, 2, 2, 1, 9, 5, 9, 8, 0, 0, 1, 1, 2, 9, 0, 0, 2, 2, 5, 1,\n",
            "        1, 2, 6, 1, 0, 4, 6, 7, 1, 4, 3, 3, 2, 4, 7, 5, 6, 8, 2, 8, 9, 2, 8, 5,\n",
            "        9, 8, 3, 9, 0, 0, 3, 2, 1, 2, 8, 8, 0, 4, 2, 6, 1, 6, 5, 8, 1, 3, 5, 7,\n",
            "        4, 3, 2, 2, 4, 9, 1, 6, 7, 8, 7, 4, 3, 5, 6, 2, 5, 5, 9, 9, 8, 7, 0, 8,\n",
            "        5, 2, 0, 3, 0, 9, 8, 2, 8, 8, 5, 7, 8, 4, 6, 3, 2, 9, 5, 9, 5, 9, 6, 7,\n",
            "        0, 8, 5, 6, 1, 8, 4, 8, 2, 0, 6, 2, 3, 1, 2, 8, 0, 6, 0, 6, 0, 2, 4, 7,\n",
            "        6, 1, 5, 5, 6, 2, 7, 7])\n",
            "512\n",
            "tensor([5, 9, 3, 3, 4, 3, 0, 7, 9, 6, 7, 1, 8, 3, 9, 9, 7, 4, 5, 9, 7, 3, 9, 0,\n",
            "        0, 1, 8, 1, 6, 8, 9, 0, 3, 0, 2, 7, 9, 3, 7, 4, 3, 4, 8, 6, 9, 0, 7, 7,\n",
            "        8, 0, 6, 5, 5, 4, 9, 0, 9, 5, 5, 3, 4, 8, 5, 4, 0, 1, 1, 9, 2, 1, 3, 9,\n",
            "        0, 9, 5, 6, 6, 2, 7, 8, 4, 1, 5, 3, 4, 4, 7, 4, 0, 1, 5, 3, 2, 7, 1, 7,\n",
            "        1, 1, 4, 5, 7, 2, 4, 2, 8, 5, 0, 9, 7, 2, 2, 7, 6, 3, 1, 8, 8, 5, 5, 1,\n",
            "        7, 5, 3, 1, 4, 0, 0, 9, 4, 4, 7, 0, 0, 4, 4, 1, 1, 0, 3, 9, 7, 4, 2, 4,\n",
            "        6, 0, 8, 6, 1, 0, 5, 2, 8, 6, 1, 9, 5, 7, 8, 5, 9, 8, 7, 8, 6, 3, 7, 8,\n",
            "        6, 4, 5, 5, 2, 0, 3, 0, 4, 5, 7, 2, 0, 2, 4, 8, 1, 6, 5, 1, 4, 6, 6, 4,\n",
            "        4, 5, 4, 2, 2, 5, 0, 3, 8, 3, 7, 1, 0, 3, 8, 1, 6, 7, 8, 4, 3, 5, 3, 8,\n",
            "        4, 3, 8, 5, 0, 0, 6, 9, 3, 8, 9, 0, 9, 6, 3, 0, 7, 9, 1, 5, 8, 7, 8, 0,\n",
            "        7, 8, 6, 5, 7, 0, 6, 4, 7, 9, 4, 2, 9, 6, 3, 0, 0, 1, 2, 5, 2, 6, 6, 1,\n",
            "        2, 6, 0, 7, 8, 6, 1, 7, 5, 0, 1, 8, 0, 4, 6, 9, 6, 7, 6, 0, 5, 7, 3, 5,\n",
            "        1, 7, 4, 1, 3, 3, 0, 4, 4, 5, 3, 4, 9, 2, 6, 0, 3, 0, 1, 8, 6, 5, 1, 7,\n",
            "        6, 3, 1, 8, 8, 6, 9, 7, 1, 9, 0, 4, 1, 7, 5, 6, 6, 5, 0, 6, 2, 1, 4, 8,\n",
            "        7, 3, 6, 6, 7, 2, 0, 8, 6, 7, 2, 8, 1, 6, 0, 5, 4, 0, 0, 4, 1, 3, 6, 0,\n",
            "        7, 4, 4, 6, 8, 7, 1, 9, 1, 7, 3, 2, 9, 0, 7, 3, 4, 3, 8, 9, 5, 6, 7, 5,\n",
            "        4, 8, 5, 1, 5, 5, 0, 3, 4, 8, 0, 1, 3, 1, 9, 3, 9, 4, 5, 2, 3, 6, 1, 9,\n",
            "        2, 4, 1, 0, 0, 1, 0, 4, 1, 2, 0, 7, 6, 7, 0, 1, 7, 4, 6, 7, 6, 4, 8, 0,\n",
            "        9, 7, 4, 8, 3, 0, 9, 6, 2, 8, 1, 5, 4, 0, 9, 5, 3, 1, 0, 8, 8, 8, 1, 5,\n",
            "        7, 6, 5, 5, 7, 4, 6, 3, 2, 4, 1, 4, 0, 2, 5, 1, 4, 0, 5, 6, 8, 3, 5, 8,\n",
            "        4, 5, 0, 7, 5, 8, 5, 4, 0, 8, 7, 5, 1, 5, 7, 0, 4, 4, 7, 0, 8, 1, 2, 9,\n",
            "        1, 7, 6, 3, 2, 4, 6, 5])\n",
            "512\n",
            "tensor([6, 1, 8, 2, 8, 0, 1, 3, 2, 3, 4, 2, 2, 1, 6, 9, 8, 4, 0, 9, 7, 9, 6, 1,\n",
            "        4, 4, 6, 5, 2, 1, 1, 2, 5, 9, 7, 9, 2, 9, 5, 6, 0, 8, 5, 7, 6, 0, 0, 7,\n",
            "        6, 8, 7, 7, 4, 7, 4, 9, 3, 7, 2, 3, 0, 3, 6, 5, 1, 9, 0, 7, 8, 4, 2, 4,\n",
            "        1, 6, 4, 3, 6, 5, 8, 1, 5, 1, 5, 9, 0, 9, 5, 3, 3, 5, 6, 8, 3, 2, 3, 7,\n",
            "        8, 7, 4, 8, 2, 9, 3, 0, 9, 9, 5, 9, 0, 8, 8, 3, 8, 5, 4, 8, 3, 7, 1, 3,\n",
            "        0, 9, 8, 3, 0, 5, 1, 6, 5, 8, 3, 7, 2, 3, 3, 7, 9, 0, 2, 6, 3, 5, 2, 4,\n",
            "        5, 0, 5, 6, 4, 8, 6, 5, 3, 4, 1, 8, 4, 9, 9, 6, 8, 0, 6, 1, 7, 5, 5, 8,\n",
            "        5, 1, 7, 4, 5, 4, 3, 4, 9, 3, 8, 7, 7, 6, 0, 6, 8, 3, 8, 8, 1, 6, 6, 8,\n",
            "        3, 3, 0, 3, 9, 6, 3, 9, 7, 5, 6, 9, 4, 4, 7, 8, 5, 0, 5, 4, 9, 0, 7, 2,\n",
            "        9, 0, 3, 5, 5, 9, 7, 2, 7, 8, 0, 2, 9, 3, 2, 5, 8, 1, 2, 1, 3, 7, 2, 2,\n",
            "        1, 0, 9, 8, 9, 0, 1, 3, 6, 4, 1, 4, 6, 6, 9, 9, 6, 9, 7, 1, 2, 4, 0, 4,\n",
            "        7, 6, 4, 7, 3, 9, 1, 4, 2, 9, 8, 7, 6, 8, 9, 5, 1, 4, 8, 1, 0, 4, 6, 0,\n",
            "        1, 2, 2, 3, 0, 4, 8, 0, 1, 8, 3, 8, 3, 7, 4, 9, 6, 7, 4, 1, 6, 7, 0, 1,\n",
            "        0, 5, 7, 6, 7, 8, 6, 5, 7, 8, 8, 7, 6, 0, 9, 9, 3, 7, 5, 5, 0, 1, 4, 9,\n",
            "        6, 0, 4, 5, 6, 4, 7, 1, 0, 1, 9, 8, 6, 4, 8, 7, 9, 1, 4, 8, 4, 3, 3, 6,\n",
            "        2, 6, 2, 1, 1, 5, 2, 3, 4, 6, 6, 5, 7, 7, 2, 6, 0, 7, 8, 1, 2, 9, 4, 3,\n",
            "        0, 8, 6, 9, 6, 4, 3, 9, 6, 1, 3, 0, 6, 9, 0, 6, 6, 8, 2, 0, 4, 6, 8, 4,\n",
            "        8, 5, 6, 1, 1, 5, 8, 9, 7, 8, 6, 5, 6, 8, 6, 6, 0, 5, 4, 2, 6, 5, 6, 2,\n",
            "        1, 8, 7, 3, 2, 5, 2, 2, 8, 7, 6, 6, 5, 6, 6, 7, 7, 9, 4, 7, 4, 5, 3, 7,\n",
            "        2, 5, 6, 9, 2, 0, 3, 4, 1, 4, 7, 3, 9, 2, 4, 8, 4, 6, 0, 6, 4, 6, 3, 4,\n",
            "        8, 6, 7, 3, 2, 3, 0, 0, 5, 7, 3, 3, 3, 3, 9, 1, 9, 5, 5, 0, 0, 6, 4, 8,\n",
            "        6, 4, 4, 2, 4, 5, 8, 0])\n",
            "512\n",
            "tensor([1, 2, 9, 2, 3, 9, 5, 2, 0, 1, 6, 5, 9, 5, 3, 4, 4, 5, 0, 0, 4, 1, 5, 0,\n",
            "        2, 6, 0, 2, 0, 6, 9, 8, 7, 4, 0, 1, 4, 2, 6, 5, 7, 5, 2, 0, 9, 6, 5, 1,\n",
            "        8, 0, 1, 3, 2, 4, 4, 6, 9, 7, 9, 6, 3, 3, 7, 0, 9, 8, 1, 8, 9, 0, 0, 4,\n",
            "        0, 8, 8, 3, 7, 5, 6, 3, 8, 0, 0, 0, 9, 3, 0, 3, 9, 7, 5, 9, 6, 5, 8, 3,\n",
            "        9, 2, 5, 2, 9, 1, 4, 0, 3, 1, 0, 2, 1, 7, 9, 9, 0, 6, 5, 5, 1, 6, 3, 3,\n",
            "        4, 5, 2, 7, 8, 5, 7, 5, 9, 8, 3, 6, 3, 7, 7, 1, 1, 6, 4, 7, 0, 2, 8, 7,\n",
            "        6, 7, 1, 7, 3, 4, 2, 9, 8, 6, 2, 8, 1, 0, 7, 2, 5, 3, 9, 2, 0, 1, 2, 8,\n",
            "        0, 1, 4, 3, 7, 6, 4, 7, 5, 3, 8, 5, 2, 6, 5, 6, 1, 7, 1, 1, 0, 2, 9, 3,\n",
            "        4, 8, 5, 5, 3, 4, 3, 4, 4, 8, 1, 1, 7, 4, 7, 5, 5, 3, 6, 7, 2, 4, 6, 4,\n",
            "        4, 7, 1, 1, 6, 7, 6, 0, 6, 5, 7, 0, 2, 7, 1, 9, 8, 7, 4, 1, 4, 8, 9, 5,\n",
            "        0, 5, 3, 9, 7, 1, 0, 3, 3, 5, 6, 0, 2, 0, 5, 1, 7, 1, 8, 3, 8, 9, 0, 0,\n",
            "        9, 9, 1, 1, 3, 4, 0, 0, 0, 8, 1, 3, 9, 3, 4, 8, 5, 1, 7, 4, 0, 8, 7, 4,\n",
            "        6, 1, 4, 2, 1, 3, 4, 8, 5, 8, 7, 5, 4, 3, 7, 2, 3, 2, 9, 2, 5, 4, 2, 1,\n",
            "        4, 6, 2, 6, 8, 2, 8, 5, 6, 2, 9, 1, 5, 8, 1, 7, 5, 1, 4, 3, 1, 0, 9, 1,\n",
            "        7, 3, 9, 6, 5, 4, 4, 6, 0, 4, 2, 4, 2, 1, 3, 1, 7, 4, 9, 3, 8, 4, 6, 9,\n",
            "        0, 9, 5, 3, 8, 2, 6, 5, 4, 9, 0, 0, 6, 8, 2, 7, 1, 2, 2, 6, 9, 1, 1, 8,\n",
            "        9, 2, 4, 2, 6, 9, 2, 8, 1, 2, 9, 3, 8, 7, 6, 6, 0, 2, 8, 2, 7, 0, 4, 8,\n",
            "        9, 8, 1, 7, 7, 0, 4, 1, 8, 1, 1, 0, 7, 4, 1, 1, 5, 5, 8, 4, 4, 2, 0, 3,\n",
            "        0, 4, 5, 9, 3, 4, 5, 8, 2, 8, 6, 0, 8, 4, 8, 3, 9, 8, 2, 7, 4, 8, 4, 7,\n",
            "        6, 9, 7, 8, 6, 9, 8, 5, 7, 6, 3, 8, 4, 0, 1, 1, 2, 8, 1, 4, 1, 1, 0, 0,\n",
            "        5, 2, 3, 3, 7, 0, 5, 2, 4, 5, 9, 2, 7, 9, 2, 4, 9, 8, 2, 2, 6, 1, 5, 6,\n",
            "        3, 7, 6, 1, 7, 2, 6, 6])\n",
            "512\n",
            "tensor([2, 7, 2, 1, 6, 7, 5, 3, 6, 7, 9, 6, 0, 7, 9, 8, 8, 1, 0, 1, 1, 4, 9, 6,\n",
            "        6, 2, 1, 3, 1, 0, 2, 8, 1, 8, 7, 8, 4, 7, 2, 6, 7, 0, 4, 5, 3, 4, 1, 5,\n",
            "        3, 5, 7, 3, 4, 8, 3, 3, 5, 4, 1, 5, 0, 9, 8, 7, 1, 2, 4, 8, 2, 1, 2, 9,\n",
            "        8, 2, 5, 8, 1, 1, 1, 6, 6, 6, 6, 0, 7, 2, 1, 7, 8, 3, 9, 6, 6, 4, 1, 3,\n",
            "        2, 7, 0, 8, 0, 8, 2, 6, 9, 2, 7, 1, 2, 3, 1, 4, 9, 1, 2, 4, 0, 1, 6, 1,\n",
            "        1, 9, 9, 4, 5, 8, 3, 6, 3, 0, 2, 5, 2, 8, 3, 0, 9, 4, 2, 8, 4, 7, 9, 7,\n",
            "        4, 3, 4, 5, 2, 3, 6, 2, 8, 4, 9, 4, 8, 1, 0, 3, 2, 8, 2, 6, 7, 2, 7, 3,\n",
            "        5, 9, 1, 6, 1, 9, 1, 1, 2, 5, 3, 9, 3, 9, 5, 5, 5, 9, 7, 4, 5, 2, 5, 2,\n",
            "        3, 7, 4, 7, 0, 9, 7, 9, 7, 9, 1, 1, 6, 8, 9, 6, 6, 0, 7, 7, 4, 1, 1, 7,\n",
            "        4, 2, 2, 4, 5, 9, 7, 1, 8, 6, 8, 3, 2, 6, 3, 5, 2, 8, 5, 3, 5, 1, 6, 0,\n",
            "        6, 0, 6, 0, 2, 9, 9, 3, 6, 4, 6, 1, 9, 7, 9, 2, 4, 3, 8, 2, 7, 1, 5, 3,\n",
            "        0, 8, 0, 3, 9, 2, 5, 1, 5, 0, 6, 7, 9, 5, 7, 3, 1, 5, 9, 7, 2, 1, 7, 7,\n",
            "        9, 7, 9, 9, 9, 8, 6, 4, 2, 9, 8, 7, 2, 7, 1, 6, 0, 7, 7, 4, 6, 2, 2, 4,\n",
            "        5, 4, 8, 3, 3, 5, 7, 6, 0, 7, 2, 8, 6, 5, 7, 5, 1, 5, 0, 6, 8, 7, 9, 8,\n",
            "        5, 6, 4, 7, 6, 9, 9, 5, 1, 3, 5, 6, 3, 1, 1, 4, 6, 0, 2, 3, 7, 1, 6, 3,\n",
            "        5, 7, 9, 6, 5, 7, 2, 8, 1, 2, 8, 2, 3, 3, 2, 2, 7, 0, 9, 1, 3, 5, 9, 5,\n",
            "        3, 7, 5, 3, 1, 6, 0, 3, 1, 6, 0, 3, 7, 6, 5, 8, 2, 5, 0, 0, 2, 2, 4, 5,\n",
            "        1, 0, 4, 0, 1, 8, 3, 4, 9, 8, 5, 0, 1, 7, 7, 6, 1, 5, 8, 4, 2, 1, 9, 8,\n",
            "        5, 1, 6, 9, 7, 3, 1, 7, 6, 2, 0, 0, 4, 0, 9, 0, 9, 8, 8, 5, 7, 7, 0, 3,\n",
            "        6, 4, 2, 4, 5, 9, 4, 5, 7, 9, 6, 4, 4, 2, 2, 6, 9, 9, 3, 2, 8, 4, 3, 1,\n",
            "        1, 9, 5, 2, 3, 6, 6, 3, 5, 0, 9, 3, 5, 0, 2, 9, 9, 2, 2, 7, 5, 4, 9, 1,\n",
            "        8, 9, 2, 5, 5, 8, 3, 9])\n",
            "512\n",
            "tensor([3, 1, 8, 9, 5, 1, 7, 8, 2, 9, 2, 6, 6, 5, 5, 8, 6, 7, 1, 5, 1, 1, 5, 0,\n",
            "        4, 5, 3, 6, 9, 2, 5, 0, 0, 3, 4, 6, 3, 4, 7, 5, 0, 6, 0, 5, 0, 8, 9, 6,\n",
            "        1, 3, 5, 2, 1, 6, 8, 0, 1, 9, 6, 8, 8, 7, 8, 9, 9, 0, 9, 1, 2, 0, 3, 3,\n",
            "        3, 8, 9, 7, 9, 7, 1, 8, 2, 2, 3, 7, 7, 9, 0, 6, 9, 5, 9, 6, 3, 5, 4, 0,\n",
            "        7, 2, 3, 3, 4, 5, 1, 5, 8, 0, 5, 6, 5, 1, 6, 4, 6, 0, 6, 5, 3, 3, 1, 3,\n",
            "        4, 6, 3, 5, 4, 9, 6, 1, 3, 0, 7, 7, 0, 6, 0, 8, 5, 9, 8, 2, 5, 1, 1, 0,\n",
            "        7, 3, 8, 0, 5, 3, 8, 5, 1, 5, 0, 7, 2, 2, 3, 8, 4, 3, 6, 8, 4, 0, 9, 0,\n",
            "        6, 7, 2, 2, 6, 6, 8, 9, 1, 7, 7, 9, 2, 1, 1, 4, 1, 2, 1, 1, 4, 6, 8, 7,\n",
            "        2, 7, 9, 7, 7, 8, 7, 8, 3, 4, 1, 5, 5, 4, 5, 3, 3, 1, 9, 9, 5, 4, 9, 7,\n",
            "        4, 7, 4, 2, 2, 3, 5, 3, 0, 0, 5, 3, 4, 3, 8, 4, 5, 6, 3, 7, 8, 9, 2, 6,\n",
            "        4, 9, 3, 5, 7, 6, 8, 6, 6, 1, 0, 3, 7, 3, 3, 5, 4, 4, 5, 3, 9, 1, 9, 0,\n",
            "        6, 7, 8, 6, 2, 9, 1, 9, 9, 5, 0, 8, 1, 0, 5, 1, 4, 7, 0, 3, 1, 2, 8, 2,\n",
            "        0, 6, 3, 4, 6, 9, 2, 9, 9, 4, 5, 3, 8, 8, 4, 0, 4, 8, 5, 1, 2, 7, 1, 6,\n",
            "        2, 1, 4, 3, 7, 2, 8, 7, 4, 2, 8, 9, 6, 7, 6, 5, 5, 8, 5, 4, 9, 0, 5, 7,\n",
            "        0, 9, 7, 1, 2, 8, 1, 6, 3, 9, 2, 7, 4, 5, 8, 4, 1, 3, 6, 8, 1, 1, 2, 6,\n",
            "        5, 7, 6, 0, 2, 7, 4, 2, 7, 8, 4, 8, 5, 5, 4, 9, 8, 7, 6, 0, 9, 4, 5, 9,\n",
            "        7, 9, 3, 2, 6, 0, 4, 0, 2, 2, 3, 5, 3, 9, 3, 4, 6, 5, 1, 7, 6, 4, 9, 4,\n",
            "        3, 3, 7, 2, 4, 7, 2, 1, 7, 5, 6, 0, 0, 3, 5, 4, 8, 8, 1, 8, 1, 7, 6, 8,\n",
            "        1, 0, 0, 7, 1, 5, 8, 1, 9, 5, 1, 5, 1, 9, 3, 6, 7, 2, 5, 9, 0, 1, 5, 7,\n",
            "        5, 6, 5, 9, 9, 2, 1, 9, 7, 1, 8, 7, 9, 7, 0, 9, 5, 5, 3, 5, 7, 1, 1, 8,\n",
            "        9, 3, 7, 0, 2, 2, 4, 4, 9, 2, 9, 7, 6, 0, 9, 2, 0, 6, 7, 9, 6, 3, 4, 0,\n",
            "        4, 1, 0, 2, 8, 5, 0, 7])\n",
            "512\n",
            "tensor([8, 2, 8, 5, 5, 3, 3, 6, 7, 8, 4, 7, 5, 4, 6, 8, 4, 3, 8, 7, 0, 7, 7, 9,\n",
            "        1, 4, 9, 2, 5, 8, 2, 6, 7, 0, 4, 7, 0, 7, 8, 2, 1, 2, 8, 4, 7, 5, 2, 6,\n",
            "        5, 2, 7, 0, 0, 0, 1, 7, 1, 5, 9, 2, 2, 8, 7, 8, 9, 7, 7, 3, 1, 4, 6, 7,\n",
            "        9, 3, 5, 2, 9, 0, 6, 9, 3, 4, 1, 3, 3, 5, 3, 8, 4, 9, 9, 2, 7, 9, 9, 6,\n",
            "        6, 6, 4, 3, 8, 8, 2, 7, 2, 3, 9, 1, 1, 9, 4, 7, 3, 4, 7, 0, 7, 9, 7, 7,\n",
            "        8, 8, 8, 1, 8, 2, 3, 5, 4, 5, 4, 5, 4, 1, 2, 0, 0, 0, 7, 8, 2, 3, 5, 7,\n",
            "        7, 7, 1, 3, 8, 1, 8, 2, 0, 2, 0, 0, 4, 3, 1, 5, 2, 8, 1, 0, 4, 0, 5, 7,\n",
            "        1, 4, 1, 8, 0, 5, 7, 6, 3, 7, 7, 2, 3, 7, 0, 0, 6, 5, 1, 6, 9, 4, 5, 2,\n",
            "        9, 7, 1, 5, 3, 3, 6, 6, 0, 7, 7, 8, 6, 8, 8, 9, 7, 5, 4, 6, 1, 2, 0, 2,\n",
            "        0, 1, 0, 5, 7, 2, 0, 6, 9, 3, 0, 3, 3, 1, 8, 1, 7, 0, 6, 8, 8, 6, 2, 3,\n",
            "        1, 7, 4, 2, 1, 2, 8, 1, 7, 6, 5, 9, 6, 6, 5, 9, 4, 3, 7, 3, 3, 4, 2, 0,\n",
            "        7, 2, 6, 1, 9, 9, 7, 5, 2, 9, 3, 2, 7, 8, 5, 5, 8, 7, 8, 3, 2, 9, 1, 4,\n",
            "        3, 0, 6, 7, 9, 5, 1, 5, 0, 2, 1, 4, 9, 0, 2, 4, 3, 3, 2, 8, 9, 8, 0, 7,\n",
            "        0, 6, 0, 9, 7, 9, 6, 6, 0, 4, 5, 3, 6, 7, 7, 2, 7, 2, 3, 3, 0, 7, 2, 9,\n",
            "        7, 5, 8, 9, 7, 2, 7, 6, 3, 7, 6, 5, 3, 5, 6, 7, 9, 9, 4, 7, 2, 7, 6, 6,\n",
            "        2, 7, 4, 2, 0, 1, 6, 2, 0, 7, 4, 8, 4, 0, 7, 4, 5, 2, 7, 8, 3, 2, 9, 1,\n",
            "        6, 2, 5, 5, 6, 7, 4, 2, 1, 2, 3, 3, 8, 0, 3, 3, 1, 9, 4, 7, 8, 9, 1, 2,\n",
            "        4, 0, 2, 9, 8, 5, 5, 0, 4, 3, 5, 2, 3, 8, 0, 6, 3, 3, 3, 3, 0, 9, 9, 4,\n",
            "        3, 5, 3, 7, 6, 5, 4, 7, 0, 0, 8, 5, 0, 8, 2, 1, 2, 1, 1, 5, 0, 4, 4, 6,\n",
            "        8, 3, 7, 3, 9, 2, 7, 4, 3, 5, 0, 8, 8, 4, 7, 3, 7, 9, 0, 5, 5, 9, 7, 8,\n",
            "        9, 6, 9, 9, 1, 3, 7, 1, 3, 1, 5, 1, 2, 5, 9, 9, 8, 8, 4, 0, 4, 9, 2, 8,\n",
            "        1, 4, 1, 6, 8, 0, 4, 1])\n",
            "512\n",
            "tensor([7, 8, 7, 3, 6, 8, 6, 1, 9, 3, 1, 9, 5, 7, 6, 3, 3, 1, 4, 8, 3, 5, 1, 6,\n",
            "        3, 9, 4, 4, 1, 8, 5, 5, 7, 1, 2, 9, 6, 9, 4, 0, 4, 1, 4, 7, 0, 0, 3, 0,\n",
            "        7, 8, 2, 4, 0, 3, 4, 3, 5, 7, 1, 3, 9, 9, 7, 6, 6, 3, 8, 5, 0, 2, 0, 5,\n",
            "        0, 6, 2, 6, 0, 1, 0, 0, 3, 6, 4, 9, 1, 5, 5, 9, 4, 7, 3, 0, 1, 9, 2, 3,\n",
            "        4, 5, 8, 2, 6, 2, 9, 0, 9, 5, 8, 3, 7, 8, 2, 3, 3, 5, 8, 2, 4, 2, 4, 7,\n",
            "        1, 3, 2, 7, 1, 1, 6, 2, 6, 8, 5, 1, 9, 9, 7, 3, 1, 5, 6, 7, 9, 5, 2, 3,\n",
            "        3, 5, 4, 5, 4, 5, 7, 4, 9, 9, 0, 5, 6, 6, 5, 3, 6, 9, 6, 2, 5, 6, 8, 2,\n",
            "        3, 4, 6, 2, 3, 8, 3, 1, 0, 0, 8, 3, 1, 1, 1, 9, 2, 0, 8, 8, 0, 3, 6, 0,\n",
            "        4, 5, 4, 3, 0, 8, 9, 6, 4, 7, 2, 9, 9, 2, 3, 4, 6, 9, 4, 7, 6, 9, 0, 5,\n",
            "        0, 2, 2, 9, 4, 7, 1, 2, 8, 2, 8, 3, 0, 3, 0, 9, 0, 5, 0, 9, 5, 8, 5, 0,\n",
            "        3, 8, 2, 0, 3, 6, 5, 4, 9, 2, 9, 3, 7, 6, 2, 3, 8, 3, 1, 2, 8, 8, 1, 6,\n",
            "        4, 7, 6, 1, 1, 3, 1, 1, 5, 9, 6, 9, 3, 5, 7, 1, 1, 7, 9, 1, 3, 5, 6, 6,\n",
            "        2, 0, 7, 0, 5, 1, 5, 7, 0, 2, 4, 4, 2, 0, 9, 9, 2, 6, 3, 3, 1, 4, 2, 1,\n",
            "        5, 5, 3, 6, 4, 0, 4, 3, 5, 7, 1, 3, 8, 7, 4, 2, 5, 2, 3, 6, 8, 4, 4, 0,\n",
            "        5, 7, 2, 2, 9, 7, 0, 5, 4, 0, 0, 5, 9, 7, 7, 4, 0, 7, 9, 4, 1, 1, 1, 5,\n",
            "        5, 3, 5, 5, 8, 2, 5, 0, 2, 2, 8, 6, 0, 0, 8, 5, 7, 9, 9, 7, 1, 9, 8, 9,\n",
            "        2, 3, 1, 0, 3, 5, 9, 0, 3, 5, 9, 7, 1, 8, 5, 5, 2, 9, 9, 0, 4, 1, 4, 4,\n",
            "        1, 3, 6, 5, 0, 4, 1, 8, 1, 7, 3, 8, 2, 8, 6, 4, 3, 9, 1, 6, 8, 1, 0, 8,\n",
            "        0, 1, 8, 4, 2, 6, 0, 4, 5, 2, 1, 0, 5, 2, 1, 0, 6, 4, 0, 4, 1, 6, 7, 3,\n",
            "        8, 7, 2, 2, 5, 3, 2, 4, 4, 7, 8, 1, 8, 3, 5, 4, 6, 8, 1, 6, 2, 5, 9, 7,\n",
            "        0, 1, 8, 3, 9, 6, 5, 4, 4, 7, 0, 3, 6, 1, 4, 7, 1, 8, 8, 1, 9, 5, 7, 7,\n",
            "        5, 7, 0, 8, 7, 2, 5, 5])\n",
            "512\n",
            "tensor([2, 2, 3, 1, 6, 1, 2, 3, 7, 0, 5, 6, 9, 5, 1, 5, 1, 5, 3, 5, 0, 6, 1, 4,\n",
            "        5, 9, 6, 5, 0, 1, 6, 6, 2, 8, 6, 3, 2, 7, 7, 0, 4, 3, 0, 8, 4, 2, 2, 9,\n",
            "        0, 1, 9, 5, 3, 7, 7, 9, 0, 3, 7, 3, 8, 1, 8, 7, 4, 4, 1, 7, 2, 9, 3, 8,\n",
            "        9, 2, 9, 6, 5, 7, 1, 6, 5, 5, 0, 0, 9, 9, 7, 4, 8, 5, 6, 6, 3, 0, 7, 9,\n",
            "        1, 8, 2, 5, 9, 8, 9, 1, 4, 4, 9, 8, 2, 0, 7, 4, 0, 7, 7, 8, 6, 9, 5, 8,\n",
            "        8, 3, 8, 0, 7, 5, 8, 8, 1, 4, 5, 6, 9, 1, 6, 8, 7, 6, 2, 4, 2, 2, 7, 2,\n",
            "        9, 3, 7, 9, 0, 8, 4, 8, 5, 1, 5, 8, 1, 4, 3, 4, 0, 5, 5, 5, 4, 1, 5, 3,\n",
            "        2, 1, 7, 7, 4, 3, 1, 2, 4, 8, 7, 6, 4, 6, 2, 6, 5, 6, 6, 6, 2, 5, 7, 9,\n",
            "        9, 6, 2, 0, 0, 6, 3, 9, 8, 0, 0, 1, 4, 5, 1, 9, 1, 2, 2, 0, 2, 3, 2, 8,\n",
            "        9, 2, 2, 3, 9, 7, 1, 4, 3, 5, 3, 3, 2, 5, 2, 0, 3, 7, 3, 7, 8, 0, 6, 8,\n",
            "        3, 6, 5, 1, 2, 9, 7, 5, 0, 4, 1, 0, 0, 3, 3, 0, 7, 4, 5, 7, 0, 5, 2, 8,\n",
            "        1, 2, 8, 6, 0, 6, 0, 4, 8, 7, 0, 8, 2, 9, 1, 5, 7, 8, 0, 8, 7, 0, 0, 1,\n",
            "        2, 8, 9, 2, 9, 0, 4, 7, 6, 4, 0, 5, 1, 7, 5, 2, 9, 4, 3, 3, 2, 4, 6, 3,\n",
            "        4, 9, 4, 2, 1, 1, 5, 8, 2, 8, 2, 5, 2, 8, 7, 1, 1, 3, 4, 3, 7, 7, 7, 8,\n",
            "        6, 7, 8, 5, 3, 6, 3, 4, 7, 4, 9, 9, 0, 8, 6, 0, 7, 2, 9, 8, 2, 0, 1, 6,\n",
            "        4, 4, 1, 1, 1, 5, 7, 9, 8, 0, 2, 0, 7, 2, 8, 0, 8, 6, 6, 8, 4, 0, 1, 7,\n",
            "        5, 2, 4, 9, 5, 8, 0, 8, 8, 8, 7, 1, 7, 0, 0, 8, 5, 3, 5, 2, 5, 1, 5, 1,\n",
            "        1, 0, 9, 1, 9, 7, 5, 6, 3, 9, 7, 1, 8, 5, 2, 9, 7, 3, 8, 8, 7, 9, 7, 3,\n",
            "        3, 8, 1, 7, 7, 6, 1, 6, 5, 4, 3, 9, 8, 9, 9, 3, 0, 4, 9, 4, 3, 5, 6, 5,\n",
            "        6, 4, 1, 1, 2, 3, 6, 7, 8, 7, 6, 8, 3, 1, 7, 7, 8, 1, 1, 4, 2, 5, 9, 1,\n",
            "        0, 4, 8, 9, 2, 9, 5, 9, 3, 4, 6, 5, 0, 1, 4, 4, 1, 2, 3, 6, 1, 8, 8, 4,\n",
            "        9, 7, 5, 5, 6, 6, 9, 4])\n",
            "512\n",
            "tensor([1, 0, 6, 2, 3, 5, 7, 7, 6, 5, 8, 5, 7, 9, 0, 8, 5, 1, 4, 7, 8, 2, 6, 9,\n",
            "        6, 3, 2, 1, 3, 4, 2, 2, 1, 8, 7, 8, 8, 2, 5, 6, 1, 4, 0, 9, 0, 0, 2, 5,\n",
            "        0, 9, 6, 7, 1, 6, 1, 4, 7, 7, 2, 5, 5, 5, 1, 0, 3, 0, 9, 1, 3, 5, 2, 8,\n",
            "        9, 9, 2, 0, 0, 6, 4, 0, 7, 9, 1, 5, 7, 7, 0, 3, 6, 1, 2, 4, 4, 8, 4, 1,\n",
            "        3, 6, 9, 6, 8, 5, 1, 7, 2, 7, 5, 9, 9, 4, 2, 3, 2, 6, 8, 4, 2, 0, 5, 9,\n",
            "        2, 5, 9, 7, 5, 9, 5, 4, 8, 1, 3, 2, 7, 7, 0, 8, 8, 9, 9, 3, 4, 1, 6, 0,\n",
            "        6, 8, 0, 2, 4, 7, 0, 4, 4, 3, 7, 6, 7, 8, 1, 4, 9, 9, 2, 2, 0, 9, 1, 5,\n",
            "        7, 6, 9, 6, 9, 6, 6, 1, 1, 6, 2, 4, 1, 1, 1, 1, 5, 8, 5, 6, 5, 5, 8, 2,\n",
            "        8, 0, 8, 8, 1, 0, 6, 9, 8, 7, 3, 0, 1, 3, 8, 5, 5, 4, 2, 2, 8, 8, 6, 8,\n",
            "        9, 6, 2, 6, 0, 6, 3, 2, 2, 0, 8, 9, 2, 4, 9, 0, 9, 4, 4, 6, 4, 9, 3, 6,\n",
            "        4, 1, 2, 0, 0, 9, 3, 5, 3, 7, 3, 5, 5, 9, 1, 4, 4, 9, 9, 4, 1, 0, 6, 4,\n",
            "        2, 3, 6, 8, 4, 0, 5, 1, 6, 9, 5, 3, 0, 6, 1, 3, 5, 7, 0, 5, 7, 8, 1, 6,\n",
            "        7, 8, 7, 6, 1, 0, 3, 4, 9, 1, 9, 0, 6, 4, 6, 7, 7, 8, 1, 4, 9, 4, 8, 8,\n",
            "        4, 9, 2, 7, 9, 6, 7, 4, 3, 3, 7, 1, 3, 6, 1, 8, 0, 6, 3, 4, 7, 2, 0, 1,\n",
            "        4, 1, 5, 2, 9, 1, 8, 0, 4, 1, 3, 5, 0, 7, 1, 6, 5, 3, 0, 9, 9, 2, 3, 6,\n",
            "        6, 0, 7, 9, 0, 6, 9, 6, 4, 8, 5, 6, 3, 2, 7, 8, 9, 7, 8, 8, 7, 3, 8, 2,\n",
            "        4, 6, 9, 4, 9, 5, 0, 2, 7, 3, 1, 8, 3, 3, 8, 3, 7, 3, 9, 6, 2, 0, 3, 0,\n",
            "        7, 8, 1, 3, 0, 7, 9, 2, 3, 8, 3, 0, 0, 0, 9, 9, 5, 0, 8, 9, 8, 2, 4, 2,\n",
            "        3, 0, 2, 7, 1, 5, 9, 9, 2, 4, 2, 2, 4, 3, 1, 1, 8, 6, 8, 4, 8, 8, 2, 4,\n",
            "        3, 8, 8, 7, 4, 2, 2, 3, 8, 7, 3, 1, 3, 7, 2, 5, 5, 4, 6, 8, 3, 6, 1, 6,\n",
            "        8, 7, 3, 2, 0, 2, 4, 6, 6, 8, 6, 4, 2, 2, 5, 9, 5, 5, 1, 5, 8, 0, 4, 3,\n",
            "        2, 7, 5, 2, 5, 0, 9, 3])\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ASUS\\Desktop\\Image-Inpainting\\gan_image.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/Image-Inpainting/gan_image.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m batch_masked \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mclone() \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m masks)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/Image-Inpainting/gan_image.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m batch_masked \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((batch_masked, masks[:, :\u001b[39m1\u001b[39m]), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/Image-Inpainting/gan_image.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m pred \u001b[39m=\u001b[39m model(batch_masked)\u001b[39m.\u001b[39mclamp(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mdetach()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/Image-Inpainting/gan_image.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (((batch \u001b[39m-\u001b[39m pred) \u001b[39m*\u001b[39m masks)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mdetach()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/Image-Inpainting/gan_image.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m pixels \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m masks\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mdetach()\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32mc:\\Users\\ASUS\\Desktop\\Image-Inpainting\\gan_image.ipynb Cell 18\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/Image-Inpainting/gan_image.ipynb#X24sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/Image-Inpainting/gan_image.ipynb#X24sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/Image-Inpainting/gan_image.ipynb#X24sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:956\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m    951\u001b[0m num_spatial_dims \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m    952\u001b[0m output_padding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_padding(\n\u001b[0;32m    953\u001b[0m     \u001b[39minput\u001b[39m, output_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_size,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    954\u001b[0m     num_spatial_dims, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m--> 956\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv_transpose2d(\n\u001b[0;32m    957\u001b[0m     \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[0;32m    958\u001b[0m     output_padding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "model = generator\n",
        "model.eval()\n",
        "N = 512\n",
        "test_loader = iter(data.DataLoader(test, N, shuffle=True, pin_memory=False))\n",
        "print(test_loader)\n",
        "\n",
        "for batch, _ in test_loader:\n",
        "    print(batch.shape[0])\n",
        "    print(_)\n",
        "    masks, _ = gen_masks(batch.shape[0])\n",
        "    batch_masked = batch.clone() * (1 - masks)\n",
        "    batch_masked = torch.cat((batch_masked, masks[:, :1]), dim=1)\n",
        "    pred = model(batch_masked).clamp(0, 1).detach()\n",
        "\n",
        "    loss += (((batch - pred) * masks)**2).sum().detach()\n",
        "    pixels += masks.sum().detach()\n",
        "\n",
        "    inpainted = batch.clone()\n",
        "    masks_byte = masks.byte()\n",
        "    inpainted[masks_byte] = pred[masks_byte].view(-1)\n",
        "    border = torch.zeros((3, 32, 2)) + 0.2196\n",
        "\n",
        "print(loss / pixels)\n",
        "\n",
        "for i in range(batch.shape[0]):\n",
        "    imgs = [\n",
        "        #batch[i],\n",
        "        #border,\n",
        "        #batch_masked[:, :-1][i],\n",
        "        # pred[i],\n",
        "        #border,\n",
        "        inpainted[i]\n",
        "    ]\n",
        "    tmp = torch.cat(imgs, dim=2)\n",
        "    tensor_image = torch.Tensor(tmp)\n",
        "    image = tensor_image.permute(1, 2, 0).numpy() * 255\n",
        "    image = Image.fromarray(image.astype('uint8'))\n",
        "    image.save('duong_dan_cho_hinh_anh.png')\n",
        "    print(\"tmp\",tmp)\n",
        "    imshow(tmp)\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cK6gHBDTq25a"
      },
      "outputs": [],
      "source": [
        "model = torch.load('generator.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "UUmYXOMWqqhv",
        "outputId": "f968ee74-7cca-4629-b284-eb22e53063c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x000002712947F690>\n",
            "512\n",
            "tensor([9, 9, 5, 5, 1, 6, 7, 3, 0, 7, 5, 0, 3, 7, 9, 4, 3, 6, 3, 0, 6, 9, 2, 1,\n",
            "        6, 1, 2, 1, 9, 7, 1, 2, 3, 1, 8, 6, 8, 1, 9, 6, 8, 8, 9, 1, 5, 5, 8, 2,\n",
            "        6, 8, 7, 3, 0, 2, 5, 6, 0, 0, 3, 3, 4, 5, 8, 7, 4, 4, 8, 7, 4, 6, 8, 6,\n",
            "        3, 8, 5, 7, 8, 7, 6, 1, 9, 5, 5, 0, 8, 8, 1, 2, 6, 3, 3, 9, 0, 0, 7, 4,\n",
            "        2, 6, 8, 4, 4, 5, 9, 8, 2, 0, 3, 5, 4, 3, 0, 1, 6, 6, 9, 3, 5, 6, 4, 6,\n",
            "        5, 9, 5, 0, 1, 5, 1, 8, 9, 2, 4, 8, 6, 7, 0, 5, 5, 8, 5, 4, 9, 2, 0, 1,\n",
            "        9, 6, 4, 8, 2, 3, 8, 8, 9, 5, 5, 6, 2, 7, 8, 9, 9, 6, 3, 8, 9, 2, 6, 0,\n",
            "        2, 5, 6, 3, 3, 7, 3, 7, 8, 7, 2, 9, 9, 1, 8, 7, 6, 5, 5, 3, 2, 9, 0, 0,\n",
            "        3, 0, 7, 5, 5, 2, 2, 1, 6, 2, 4, 8, 0, 0, 1, 0, 6, 1, 9, 9, 0, 8, 6, 5,\n",
            "        1, 4, 7, 7, 7, 8, 4, 6, 4, 8, 7, 5, 6, 7, 0, 5, 9, 7, 4, 4, 6, 0, 7, 4,\n",
            "        2, 9, 1, 1, 5, 7, 9, 8, 9, 4, 1, 6, 4, 0, 1, 9, 8, 0, 5, 1, 5, 0, 7, 7,\n",
            "        7, 6, 6, 4, 5, 0, 4, 7, 5, 1, 1, 2, 4, 0, 4, 7, 7, 4, 5, 2, 4, 7, 6, 6,\n",
            "        5, 8, 6, 9, 3, 5, 2, 5, 3, 4, 8, 4, 2, 7, 3, 4, 3, 0, 9, 0, 5, 2, 7, 5,\n",
            "        1, 1, 8, 0, 8, 3, 4, 3, 3, 6, 0, 0, 2, 4, 3, 7, 5, 3, 8, 0, 4, 4, 6, 0,\n",
            "        7, 3, 3, 2, 5, 9, 9, 9, 2, 0, 2, 1, 3, 2, 4, 1, 2, 7, 6, 1, 6, 9, 7, 5,\n",
            "        6, 3, 9, 9, 6, 4, 5, 7, 6, 4, 5, 9, 7, 5, 7, 1, 8, 2, 3, 7, 2, 9, 0, 7,\n",
            "        7, 0, 0, 9, 4, 6, 7, 1, 7, 0, 1, 0, 3, 4, 7, 0, 2, 7, 2, 1, 7, 9, 9, 9,\n",
            "        7, 7, 4, 6, 0, 8, 3, 3, 1, 8, 8, 7, 7, 1, 6, 4, 2, 5, 8, 2, 1, 1, 4, 4,\n",
            "        1, 1, 4, 2, 8, 7, 9, 2, 8, 9, 6, 0, 7, 3, 7, 5, 4, 8, 0, 6, 0, 4, 8, 1,\n",
            "        6, 2, 1, 0, 9, 6, 1, 9, 0, 5, 5, 1, 5, 5, 2, 7, 7, 1, 1, 1, 7, 5, 3, 7,\n",
            "        4, 9, 5, 0, 6, 6, 0, 0, 9, 6, 2, 0, 1, 3, 1, 7, 8, 2, 5, 5, 9, 7, 0, 7,\n",
            "        7, 3, 1, 9, 7, 9, 8, 8])\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'Generator' object has no attribute 'gen_masks'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ASUS\\Desktop\\Image-Inpainting\\gan_image.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/Image-Inpainting/gan_image.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(batch\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/Image-Inpainting/gan_image.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(_)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/Image-Inpainting/gan_image.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m masks, _ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgen_masks(batch\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/Image-Inpainting/gan_image.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m batch_masked \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mclone() \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m masks)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/Image-Inpainting/gan_image.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m batch_masked \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((batch_masked, masks[:, :\u001b[39m1\u001b[39m]), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'Generator' object has no attribute 'gen_masks'"
          ]
        }
      ],
      "source": [
        "\n",
        "model = generator\n",
        "model.eval()\n",
        "N = 512\n",
        "test_loader = iter(data.DataLoader(test, N, shuffle=True, pin_memory=False))\n",
        "print(test_loader)\n",
        "\n",
        "for batch, _ in test_loader:\n",
        "    print(batch.shape[0])\n",
        "    print(_)\n",
        "    masks, _ = model.gen_masks(batch.shape[0])\n",
        "    batch_masked = batch.clone() * (1 - masks)\n",
        "    batch_masked = torch.cat((batch_masked, masks[:, :1]), dim=1)\n",
        "    pred = model.model(batch_masked).clamp(0, 1).detach()\n",
        "\n",
        "    loss += (((batch - pred) * masks)**2).sum().detach()\n",
        "    pixels += masks.sum().detach()\n",
        "\n",
        "    inpainted = batch.clone()\n",
        "    masks_byte = masks.byte()\n",
        "    inpainted[masks_byte] = pred[masks_byte].view(-1)\n",
        "    border = torch.zeros((3, 32, 2)) + 0.2196\n",
        "\n",
        "print(loss / pixels)\n",
        "\n",
        "for i in range(batch.shape[0]):\n",
        "    imgs = [\n",
        "        #batch[i],\n",
        "        #border,\n",
        "        #batch_masked[:, :-1][i],\n",
        "        # pred[i],\n",
        "        #border,\n",
        "        inpainted[i]\n",
        "    ]\n",
        "    tmp = torch.cat(imgs, dim=2)\n",
        "    tensor_image = torch.Tensor(tmp)\n",
        "    image = tensor_image.permute(1, 2, 0).numpy() * 255\n",
        "    image = Image.fromarray(image.astype('uint8'))\n",
        "    image.save('duong_dan_cho_hinh_anh.png')\n",
        "    print(\"tmp\",tmp)\n",
        "    imshow(tmp)\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yITH0w2EqkqQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NdCygALSuuW",
        "outputId": "b82609a9-6fa2-4f60-ae37-8d234e165555"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "model_save_name = 'generator1.pt'\n",
        "torch.save(generator.state_dict(), model_save_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJYiPzYASoea"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
